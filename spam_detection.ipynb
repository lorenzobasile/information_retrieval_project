{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web spam detection through link-based features\n",
    "\n",
    "Project for Information Retrieval exam at University of Trieste, January 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nlaFEQXfIVoY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import spam_detection as sd #python module containing custom functions\n",
    "np.random.seed(2) #random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9agASkKvIVo4"
   },
   "source": [
    "Web spam detection is a crucial issue for web search engines. In fact, ranking algorithms such as PageRank cannot explicitly penalize spam websites in favor of trustworthy ones, meaning that users may find very high-rank pages that have no useful content and are highly ranked because they are part of a link farm, a popular way to fool ranking algorithms.\n",
    "\n",
    "###  The dataset\n",
    "The [WEBSPAM-UK2006](https://chato.cl/webspam/datasets/uk2006/) dataset contains 11402 hosts in the `.uk` domain, of which 7866 are labeled as *spam* or *normal*. Newer datasets have been released by the same authors, but this 2006 version remains the one with the highest number of manually labeled samples.\n",
    "\n",
    "The file `new_hostnames.csv` contains the names of the hosts in the dataset, while `webspam-uk2006-labels.txt` assigns to 8045 host names a label chosen among *spam*, *normal* or *undecided*. For the purposes of this project, *undecided*-labeled hosts were considered unlabeled, leaving only 7866 hosts labeled as *spam* or *normal*.\n",
    "\n",
    "Finally, the file `uk-2006-05.hostgraph_weighted.txt` contains the weighted graph of the hosts, each row containing a host index, the indices of outlinked hosts and, for each of them, the number of outlinks.\n",
    "\n",
    "The function `read_graph` returns a sparse `csr_matrix` $\\mathcal{R}$, with $\\mathcal{R}_{i,j}$ equal to $0$ if there is no edge connecting host $i$ to host $j$ or to $\\frac{1}{O(i)}$ where $O(i)$ is the total number of hosts outlinked by $i$.\n",
    "\n",
    "Since PageRank algorithm requires a stochastic matrix, but there is no guarantee that each host has at least one outlink (dangling node problem), as proposed in [[1]](#references), an artificial node with a single self-loop was added to the graph, with ingoing edges from all dangling nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IX0xK75NIVo5"
   },
   "outputs": [],
   "source": [
    "hostnames=sd.hostnames_list('data/new_hostnames.csv')\n",
    "labels_dict=sd.labels_dictionary('data/webspam-uk2006/webspam-uk2006-labels.txt')\n",
    "labels, labeled_dataset=sd.make_dataset(labels_dict,hostnames)\n",
    "R=sd.read_graph('data/uk-2006-05.hostgraph_weighted.txt',len(hostnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCzgDNM_IVpO"
   },
   "source": [
    "### PageRank\n",
    "\n",
    "In the following cell PageRank is computed iteratively, according to the equation:\n",
    "$$\n",
    "{rank}_{k+1}=\\frac{\\alpha}{N}\\mathbf{1}+(1-\\alpha)R^T\\cdot {rank}_k\n",
    "$$\n",
    "where ${rank}_k$ is the column vector storing the PageRank scores at step $k$, $\\mathbf{1}$ is a column vector of ones, $N$ is the number of nodes (in this case 11403) and $\\alpha$ is the teleporting factor. The iterative computation is performed up to a fixed precision of $\\epsilon$, i.e. until $|{rank}_k-{rank}_{k-1}|_1 < \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank computation\n",
      "PageRank computed\n"
     ]
    }
   ],
   "source": [
    "alpha=.01\n",
    "eps=1e-8\n",
    "\n",
    "print(\"PageRank computation\")\n",
    "rank=sd.compute_PR(alpha,eps,R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An approximation for Personalized PageRank\n",
    "\n",
    "Personalized PageRank is an algorithm directly derived from PageRank, whose result is a $N \\times N$ matrix $\\mathcal{PRM}$ such that $\\mathcal{PRM}_{i,j}$ is the contribution of node $i$ to the PageRank of node $j$. This implies that the sum of $\\mathcal{PRM}$ is equal to the PageRank vector.\\\n",
    "The contribution vector $cpr(v)$ is defined to be the row vector whose transpose is the $v$-th column of matrix $\\mathcal{PRM}$. This vector stores the contribution of all nodes to the PageRank of node $v$ and it is of particular interest when it comes to web spam detection.\\\n",
    "\\\n",
    "However, Personalized PageRank computation is infeasible on large datasets, since it requires an iterative computation (conceptually identical to the one of PageRank) that includes, at each step, a matrix multiplication between two square $N \\times N$ matrices, one of which is non-sparse.\\\n",
    "To address this problem, the authors of [[2]](#references) propose a local algorithm for the computation of $\\delta$-approximations of contribution vectors.\n",
    "\\\n",
    "Given a node $v$ and its contribution vector $c:=cpr(v)$, a $\\delta$-approximation of $cpr(v)$ is a non-negative vector $c^*$ such that $c(u)-\\delta rank(v) \\leq c^*(u) \\leq c(u)$ for all nodes $u$.\n",
    "\n",
    "For the following computations, it is useful to store the columns of the $\\mathcal{R}$ matrix in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KLV0yehHIVo6"
   },
   "outputs": [],
   "source": [
    "columns=sd.columns_list(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hUy9RdLnIVpP",
    "outputId": "fa93a50b-58b4-476a-ed33-16ea20e10352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of Personalized PageRank for labeled hosts\n",
      "7866/7866\r"
     ]
    }
   ],
   "source": [
    "delta=1e-3\n",
    "\n",
    "nl=len(labeled_dataset)\n",
    "ap=np.zeros((nl,len(rank)))\n",
    "print(\"Approximation of Personalized PageRank for labeled hosts\")\n",
    "for v in range(nl):\n",
    "    print(str(v+1)+'/'+str(nl),end='\\r')\n",
    "    ap[v]=(sd.approximate_contributions(labeled_dataset[v], alpha, delta*rank[labeled_dataset[v]], rank[labeled_dataset[v]], columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cs7mwdBIVpb"
   },
   "source": [
    "### Features for link-based web spam detection\n",
    "Spam detection is particularly relevant for high PageRank hosts, since people tend to click on highly ranked pages, almost always within the first page of search engine results [[3]](#references).\\\n",
    "However, we can see that PageRank alone is not able to filter out spam pages. In fact, if we restrict our view to the highest ranked 25% of the dataset, 161 hosts out of 2095 (total labeled hosts in this 25%) are labeled as spam, as opposed to 773/7866 on the entire dataset. The proportion of spam hosts drops from 9.8% to 7.7%, but that is surely not enough to consider PageRank as a spam detection or spam-robust algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FGIMhttnIVpc",
    "outputId": "21caa124-fd33-4993-f4c5-c33c69a150e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spam hosts:  773\n",
      "Total labeled hosts:  7866\n",
      "Total spam hosts %:  9.827103991863718 \n",
      "\n",
      "Spam hosts in top 25%:  144\n",
      "Labeled hosts in top 25%:  2276\n",
      "Spam hosts % in top 25%:  6.3268892794376095\n"
     ]
    }
   ],
   "source": [
    "n=25\n",
    "labeled_top=sd.top_n_percent(n,rank,labeled_dataset)\n",
    "\n",
    "y=labels[labeled_dataset]\n",
    "y_top=y[labeled_top]\n",
    "\n",
    "print(\"Total spam hosts: \", sum(y))\n",
    "print(\"Total labeled hosts: \",len(y))\n",
    "print(\"Total spam hosts %: \", 100*sum(y)/len(y),'\\n')\n",
    "print(\"Spam hosts in top 25%: \", sum(y_top))\n",
    "print(\"Labeled hosts in top 25%: \", len(y_top))\n",
    "print(\"Spam hosts % in top 25%: \", 100*sum(y_top)/len(y_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic approaches to web spam detection: content-based and link-based.\\\n",
    "In a content-based setting the `HTML` code of pages is scanned and some features are computed, usually related to text (for example, average word length or fraction of visible text). On the other hand, in a link-based setting (the one explored in this notebook), information is obtained exclusively from the web graph and computations performed over it (such as ranking algorithms).\\\n",
    "Some very trivial link-based features that can be computed even before performing PageRank are indegree and outdegree, defined as follows:\n",
    " - Indegree: total number of incoming links to a host\n",
    " - Outdegree: total number of outgoing links from a host\n",
    "\n",
    "Other useful features can be computed from the contribution vector of a node (or from a $\\delta$-approximation):\n",
    " - Size of $\\delta$-significant contributing set: for a node $v$, this feature is defined as $cs\\_size=|S_{\\delta}|=|\\{u|c^*(u)>\\delta rank(v)\\}|$.\n",
    " - Contribution from vertices in the $\\delta$-significant contributing set: $cs\\_contribution=\\sum_{u \\in S_{\\delta}} \\frac{c^*(u)}{rank(v)}$ \n",
    " - $l_2$ norm of $\\delta$-significant contributing vector: $l_2\\_norm=\\sum_{u \\in S_{\\delta}} (\\frac{c^*(u)}{rank(v)})^2$\n",
    "\n",
    "These features, together with PageRank scores, can be used to train a binary classifier for spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Cnjr-v6ZIVpb"
   },
   "outputs": [],
   "source": [
    "x=np.zeros((nl,6))\n",
    "indegree,outdegree,cs_size,cs_contribution,l2_norm=sd.extract_features(R,delta,ap,labeled_dataset,rank)\n",
    "x[:,0]=rank[labeled_dataset]\n",
    "x[:,1]=indegree\n",
    "x[:,2]=outdegree\n",
    "x[:,3]=cs_size\n",
    "x[:,4]=cs_contribution\n",
    "x[:,5]=l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTE-XdduQcuW"
   },
   "source": [
    "### Evaluation of classifiers\n",
    "\n",
    "Some Machine Learning can be used to detect spam websites. The simplest approach would be to consider one individual feature, set a threshold and classify as *spam* or *normal* all hosts with a score above (or below) that threshold.\\\n",
    "Actually, despite being trivial, this approach works quite well in this case, since it allows to classify labeled hosts with an overall accuracy of 70% and, most importantly, with a recall of 95% on the *spam* class, for which one could imagine recall to be crucial over precision.\\\n",
    "However, more complex techniques can be used to increase accuracy and precision, while trying not to compromise too much recall.\\\n",
    "Simple tree-based models seem to work well on these data. In fact, a decision tree classifier scores an accuracy just below 90% and a random forest classifier scores 92% accuracy. However, one has to keep in mind that these models have much lower recall on *spam* class, scoring respectively 44% and 47%, meaning that more than 1 spam website out of 2 is classified as a *normal* website.\\\n",
    "Similar results can be achieved on the set of top 25% highest ranked hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ONBwFzjGIVpd",
    "outputId": "4c089661-8048-4cd2-db32-49c5038f2493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single split decision tree\n",
      "Accuracy:  0.7116704805491991\n",
      "Precision on spam:  0.2470389170896785\n",
      "Recall on spam:  0.944372574385511\n",
      "Decision tree\n",
      "Accuracy:  0.9092295957284515\n",
      "Precision on spam:  0.5390728476821192\n",
      "Recall on spam:  0.5265200517464425\n",
      "Random forest\n",
      "Accuracy:  0.931858632087465\n",
      "Precision on spam:  0.6908212560386473\n",
      "Recall on spam:  0.5549805950840879\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "\n",
    "print(\"Single split decision tree\")\n",
    "clf=DecisionTreeClassifier(max_depth=1,class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x,y,k)\n",
    "\n",
    "print(\"Decision tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x,y,k)\n",
    "\n",
    "print(\"Random forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced_subsample')\n",
    "sd.print_prediction_metrics(clf,x,y,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single split decision tree\n",
      "Accuracy:  0.9582601054481547\n",
      "Precision on spam:  0.6033755274261603\n",
      "Recall on spam:  0.9930555555555556\n",
      "Decision tree\n",
      "Accuracy:  0.961335676625659\n",
      "Precision on spam:  0.6609195402298851\n",
      "Recall on spam:  0.7986111111111112\n",
      "Random forest\n",
      "Accuracy:  0.9630931458699473\n",
      "Precision on spam:  0.6704545454545454\n",
      "Recall on spam:  0.8194444444444444\n"
     ]
    }
   ],
   "source": [
    "x_top=x[labeled_top]\n",
    "\n",
    "print(\"Single split decision tree\")\n",
    "clf=DecisionTreeClassifier(max_depth=1,class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x_top,y_top,k)\n",
    "\n",
    "print(\"Decision tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x_top,y_top,k)\n",
    "\n",
    "print(\"Random forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced_subsample')\n",
    "sd.print_prediction_metrics(clf,x_top,y_top,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<a id='references'></a>\n",
    "- [1] R. Andersen, C. Borgs, J. Chayes, J. Hopcroft, K. Jain, V. Mirrokni and S. Teng. Robust PageRank and locally computable spam detection features. In Proceedings of the 4th international workshop on Adversarial information retrieval on the web (AIRWeb '08), 2008.\n",
    "- [2] R. Andersen, C. Borgs, J. Chayes, J. Hopcroft, V. Mirrokni and S. Teng. Local computation of PageRank contributions. In 5th International Workshop of Algorithms and Models for the Web-Graph, 2007.\n",
    "- [3] C. Barry, M. Lardner. A Study of First Click Behaviour and User Interaction on the Google SERP. In: Pokorny J. et al. (eds) Information Systems Development. Springer, New York, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
