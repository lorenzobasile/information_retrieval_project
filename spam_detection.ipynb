{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web spam detection through link-based features\n",
    "\n",
    "Project for Information Retrieval exam at University of Trieste, January 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nlaFEQXfIVoY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import spam_detection as sd #python module containing custom functions\n",
    "np.random.seed(2) #random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9agASkKvIVo4"
   },
   "source": [
    "In the context of web search, spam is the fraudulent manipulation of web page content for the purpose of appearing high up in search results and web spam detection is a crucial issue for web search engines. In fact, ranking algorithms such as PageRank cannot explicitly penalize spam websites in favor of trustworthy ones, meaning that users may find very high-rank pages that have no useful content and are highly ranked only because they are part of a link farm, a popular way to fool ranking algorithms.\n",
    "\n",
    "###  The dataset\n",
    "The [WEBSPAM-UK2006](https://chato.cl/webspam/datasets/uk2006/) dataset contains 11402 hosts in the `.uk` domain, of which 7866 are labeled as spam or normal (non-spam). Newer datasets have been released by the same authors, but this 2006 version remains the one with the highest number of manually labeled samples.\n",
    "\n",
    "The file `new_hostnames.csv` contains the names of the hosts in the dataset, while `webspam-uk2006-labels.txt` assigns to 8045 host names a label chosen among spam, normal or undecided. For the purpose of this project, undecided-labeled hosts were considered unlabeled, leaving only 7866 hosts labeled as spam or normal.\n",
    "\n",
    "Finally, the file `uk-2006-05.hostgraph_weighted.txt` contains the weighted graph of the hosts, each row containing a host index, the indices of outlinked hosts and, for each of them, the number of outlinks.\n",
    "\n",
    "The function `read_graph` returns a sparse `csr_matrix` $R^T$, with $R[i,j]$ equal to $0$ if there is no edge connecting host $i$ to host $j$ or to $\\frac{1}{O[i]}$ where $O[i]$ is the total number of hosts outlinked by $i$.\n",
    "\n",
    "Since PageRank algorithm requires $R$ to be a stochastic matrix, but there is no guarantee that each host has at least one outlink (dangling nodes problem), as proposed in [[1]](#references), an artificial node with a single self-loop was added to the graph, with ingoing edges from all dangling nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IX0xK75NIVo5"
   },
   "outputs": [],
   "source": [
    "hostnames=sd.hostnames_list('data/new_hostnames.csv')\n",
    "labels_dict=sd.labels_dictionary('data/webspam-uk2006-labels.txt')\n",
    "labels, labeled_dataset=sd.make_dataset(labels_dict,hostnames)\n",
    "R_T=sd.read_graph('data/uk-2006-05.hostgraph_weighted.txt', len(hostnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCzgDNM_IVpO"
   },
   "source": [
    "### PageRank\n",
    "\n",
    "In the following cell PageRank is computed iteratively, according to the equation:\n",
    "\n",
    "$$\n",
    "{rank}_{k+1}=\\frac{\\alpha}{N}\\mathbf{1}+(1-\\alpha)R^T\\cdot {rank}_k\n",
    "$$\n",
    "\n",
    "where ${rank}_k$ is the column vector storing the PageRank scores at step $k$, $\\mathbf{1}$ is a column vector of ones, $N$ is the number of nodes (in this case 11403) and $\\alpha$ is the teleporting factor. The iterative computation is performed up to a fixed precision of $\\epsilon$, i.e. until $|{rank}_k-{rank}_{k-1}|_1 < \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank computation\n",
      "PageRank computed\n"
     ]
    }
   ],
   "source": [
    "alpha=.1\n",
    "eps=1e-8\n",
    "\n",
    "print(\"PageRank computation\")\n",
    "rank=sd.compute_PR(alpha, eps, R_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cs7mwdBIVpb"
   },
   "source": [
    "Spam detection is particularly relevant for high PageRank hosts, since people tend to click on highly ranked pages, almost always within the first page of search engine results [[3]](#references).\n",
    "\n",
    "However, we can see that PageRank alone is not able to filter out spam pages. In fact, if we restrict our view to the highest ranked 25% of the labeled dataset, 139 hosts out of 1966 are labeled as spam, as opposed to 773/7866 on the entire dataset. The proportion of spam hosts drops from 9.8% to 7.1%, but that is surely not enough to consider PageRank as a spam detection or spam-robust algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FGIMhttnIVpc",
    "outputId": "21caa124-fd33-4993-f4c5-c33c69a150e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spam hosts:  773\n",
      "Total labeled hosts:  7866\n",
      "Total spam hosts %:  9.827 \n",
      "\n",
      "Spam hosts in top 25%:  139\n",
      "Labeled hosts in top 25%:  1966\n",
      "Spam hosts % in top 25%:  7.07\n"
     ]
    }
   ],
   "source": [
    "n=25 #percentage of labeled dataset to consider\n",
    "nl=len(labeled_dataset)\n",
    "labeled_top=(-rank[labeled_dataset]).argsort()[:(n*nl)//100]\n",
    "y=labels\n",
    "y_top=y[labeled_top]\n",
    "\n",
    "print(\"Total spam hosts: \", sum(y))\n",
    "print(\"Total labeled hosts: \", len(y))\n",
    "print(\"Total spam hosts %: \", round(100*sum(y)/len(y), 3), '\\n')\n",
    "print(\"Spam hosts in top 25%: \", sum(y_top))\n",
    "print(\"Labeled hosts in top 25%: \", len(y_top))\n",
    "print(\"Spam hosts % in top 25%: \", round(100*sum(y_top)/len(y_top), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An approximation for PageRank contributions\n",
    "\n",
    "For the purpose of spam detection, it is useful to have a measure of the individual contributions of other hosts to the PageRank of a given host. This means obtaining a $N \\times N$ matrix $PRM$ such that $PRM[i,j]$ is the contribution of node $i$ to the PageRank of node $j$ and such that the sum over the columns of $PRM$ is equal to the PageRank vector.\n",
    "\n",
    "To do so, one possibility (explored in [[1]](#references)) is to use topic-specific PageRank $N$ times with jump vectors belonging to the canonical basis of $\\mathbb{R}^N$. With this assumption, each row of $PRM$ satisfies the equation:\n",
    "$$\n",
    "PRM[u]=\\alpha \\mathbf{e_u}+(1-\\alpha)PRM[u] \\cdot R\n",
    "$$\n",
    "Where $\\mathbf{e_u}$ is the $u$-th row vector of the canonical basis of $\\mathbb{R}^N$.\n",
    "\n",
    "Then, given a node index $v$, its corresponding contribution vector $cpr[v]$ is defined to be the $v$-th column of matrix $PRM$. This vector stores the contribution of all nodes to the PageRank of node $v$ and it will prove to be of particular interest when it comes to web spam detection.\n",
    "\n",
    "However, computing topic-specific PageRank $N$ times is infeasible on large datasets, since each step requires an iterative computation conceptually identical to the one of PageRank.\n",
    "\n",
    "To address this problem, the authors of [[2]](#references) propose a local algorithm for the computation of $\\delta$-approximations of contribution vectors.\n",
    "\n",
    "Given a node $v$ and its contribution vector $c_v:=cpr[v]$, a $\\delta$-approximation of $c_v$ is a non-negative vector $c_v^*$ such that: \n",
    "\n",
    "$$\n",
    "c_v[u]-\\delta \\cdot rank[v] \\leq c_v^*[u] \\leq c_v[u]\\space \\space \\space \\forall u \\in \\{0,...,N-1\\}\n",
    "$$\n",
    "\n",
    "For each node the algorithm runs in time $O(\\frac{1}{\\alpha \\delta})$. For $\\alpha=0.1$, the value $\\delta=0.001$ proved to be a good compromise between time efficiency and precision of approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hUy9RdLnIVpP",
    "outputId": "fa93a50b-58b4-476a-ed33-16ea20e10352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of contribution vectors for labeled hosts\n",
      "7866/7866\r"
     ]
    }
   ],
   "source": [
    "delta=1e-3\n",
    "cstar=np.zeros((nl, len(rank)))\n",
    "print(\"Approximation of contribution vectors for labeled hosts\")\n",
    "\n",
    "'''\n",
    "approximate_contributions makes repeated access to the rows of R_T. Scipy sparse matrices are much slower than numpy arrays for\n",
    "slicing, hence it is much more efficient to convert R_T to numpy for this computation. However, in the case of a larger dataset,\n",
    "this step might not be feasible because of memory limitations.\n",
    "'''\n",
    "\n",
    "R_T_np=R_T.toarray()\n",
    "for v in range(nl):\n",
    "    print(str(v+1)+'/'+str(nl), end='\\r')\n",
    "    cstar[v]=(sd.approximate_contributions(labeled_dataset[v], alpha, delta*rank[labeled_dataset[v]], rank[labeled_dataset[v]], R_T_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for link-based web spam detection\n",
    "\n",
    "There are two basic approaches to web spam detection: content-based and link-based.\n",
    "\n",
    "In a content-based setting the `HTML` code of pages is scanned and some features are computed, usually related to text (for example, average word length or fraction of visible text). On the other hand, in a link-based setting (the one explored in this notebook), information is obtained exclusively from the web graph and computations performed over it (such as ranking algorithms).\n",
    "\n",
    "Some very trivial link-based features that can be computed even before performing PageRank are indegree and outdegree, defined as follows:\n",
    " - Indegree: number of incoming links to a host\n",
    " - Outdegree: number of outgoing links from a host\n",
    "\n",
    "Other useful features can be computed from the contribution vector of a node $v$ (or from a $\\delta$-approximation of it):\n",
    " - Size of $\\delta$-significant contributing set: this feature is defined as $cs\\_size[v]=|S_{\\delta}[v]|=|\\{u : c_v^*[u]>\\delta rank[v]\\}|$.\n",
    " - Contribution from vertices in the $\\delta$-significant contributing set: $cs\\_contribution[v]=\\frac{1}{rank[v]} \\sum_{u \\in S_{\\delta}[v]} c_v^*[u]$ \n",
    " - $l_2$ norm of $\\delta$-significant contributing vector: $l_2\\_norm[v]=\\sum_{u \\in S_{\\delta}[v]} (\\frac{c_v^*[u]}{rank[v]})^2$\n",
    "\n",
    "\n",
    "These features can be used to train a binary classifier for spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cnjr-v6ZIVpb"
   },
   "outputs": [],
   "source": [
    "x=np.zeros((nl, 5))\n",
    "indegree, outdegree, cs_size, cs_contribution, l2_norm=sd.extract_features(R_T_np, delta, cstar, labeled_dataset, rank)\n",
    "x[:,0]=indegree\n",
    "x[:,1]=outdegree\n",
    "x[:,2]=cs_size\n",
    "x[:,3]=cs_contribution\n",
    "x[:,4]=l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTE-XdduQcuW"
   },
   "source": [
    "### Evaluation of classifiers\n",
    "\n",
    "Some Machine Learning can be used to detect spam websites. Here three different approaches are compared: Logistic Regression, Decision Tree and Random Forest. For all these classifiers, the implementation from `scikit-learn` library is used with its default hyperparameter values, with the exception of the parameter `class_weight`, which is set to `balanced` to try to mitigate the unbalance in the dataset between spam and normal labeled data points. \n",
    "\n",
    "The fit and test of these models are performed through a 5-fold cross validation. Regarding performance metrics, in a classification setting accuracy is obviously to be taken into account but, in this case, it is not sufficient to compare and assess the models. In fact, given how unbalanced the dataset is, even a naive classifier that classifies all hosts as normal would achieve over 90% accuracy despite being completely useless for spam detection. To solve this problem, class-specific precision and recall statistics are taken into account for spam class. In particular, since spam labeling might be safety critical, one could imagine recall to be particularly relevant.\n",
    "\n",
    "Tree-based classifiers work definitely better than Logistic Regression, scoring accuracies over 90% while attaining reasonable values for precision and recall on spam samples. However, Logistic Regression scores an outstanding 96% recall on spam meaning that it succeeds in retrieving almost all spam hosts (at the cost of many false alarms, captured by low precision score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ONBwFzjGIVpd",
    "outputId": "4c089661-8048-4cd2-db32-49c5038f2493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy:  0.71\n",
      "Precision on spam:  0.246\n",
      "Recall on spam:  0.96\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.91\n",
      "Precision on spam:  0.558\n",
      "Recall on spam:  0.554\n",
      "\n",
      "Random Forest\n",
      "Accuracy:  0.93\n",
      "Precision on spam:  0.698\n",
      "Recall on spam:  0.554\n"
     ]
    }
   ],
   "source": [
    "k=5 #k-fold CV parameter\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "clf=LogisticRegression(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf, x, y, k)\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf, x, y, k)\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf, x, y, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it is particularly interesting to restrict the view to the top 25% highest ranked labeled hosts, since these are the ones that users are most likely to run into.\n",
    "\n",
    "Here there is no huge gap in accuracy between the three classifiers, with values well over 90% for all of them. Tree-based methods confirm their higher precision on spam samples but, because of the different misclassification cost between the two class and because of a (almost) perfect recall score on spam, Logistic Regression may be preferrable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy:  0.94\n",
      "Precision on spam:  0.552\n",
      "Recall on spam:  0.993\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.96\n",
      "Precision on spam:  0.754\n",
      "Recall on spam:  0.683\n",
      "\n",
      "Random Forest\n",
      "Accuracy:  0.98\n",
      "Precision on spam:  0.831\n",
      "Recall on spam:  0.813\n"
     ]
    }
   ],
   "source": [
    "x_top=x[labeled_top]\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "clf=LogisticRegression(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf, x_top, y_top, k)\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf, x_top, y_top, k)\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf, x_top, y_top, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust PageRank\n",
    "\n",
    "PageRank contributions can be also used to define a robust version of PageRank, i.e. a ranking algorithm similar to PageRank that penalizes spam pages.\n",
    "\n",
    "From the definition of the matrix $PRM$:\n",
    "\n",
    "$$\n",
    "rank[v]=\\sum_u PRM[u,v]\n",
    "$$\n",
    "\n",
    "Robust PageRank can be defined by the following equation:\n",
    "\n",
    "$$\n",
    "robust\\_rank[v]=\\sum_u min(PRM[u,v],\\delta \\cdot rank[v])\n",
    "$$\n",
    "\n",
    "The intuition behind this definition is that spam pages tend to have a huge PageRank contribution from a very small set of nodes, while trustworthy pages have lower individual contributions but from more nodes. Then, this definition penalizes spam pages by eliminating very high values for $PRM[u,v]$ and substituting them with $\\delta \\cdot rank[v]$.\n",
    "\n",
    "Robust PageRank can be rewritten as: \n",
    "\n",
    "$$robust\\_rank[v]=\\sum_{u \\notin S_\\delta[v]} PRM[u,v]+\\sum_{u \\in S_\\delta[v]}\\delta \\cdot rank[v]=\\sum_u PRM[u,v] - \\sum_{u \\in S_\\delta[v]} PRM[u,v] +\\sum_{u \\in S_\\delta[v]}\\delta \\cdot rank[v]=\n",
    "$$\n",
    "$$\n",
    "=rank[v]- \\sum_{u \\in S_\\delta[v]} PRM[u,v]+\\delta \\cdot rank[v] \\cdot cs\\_size[v]\n",
    "$$\n",
    "\n",
    "$PRM[u,v]$ can be approximated by $c_v^*[u]$ and recalling the definition:\n",
    "\n",
    "$$\n",
    "cs\\_contribution[v]=\\frac{1}{rank[v]} \\sum_{u \\in S_{\\delta}[v]} c_v^*[u] \n",
    "$$\n",
    "It follows that:\n",
    "$$\n",
    "robust\\_rank[v]=rank[v]- cs\\_contribution[v] \\cdot rank[v]+\\delta \\cdot rank[v] \\cdot cs\\_size[v]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_rank=rank[labeled_dataset]-np.multiply(rank[labeled_dataset], cs_contribution)+delta*np.multiply(rank[labeled_dataset], cs_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modified ranking algorithm, spam hosts almost disappear from the top 25% of the labeled dataset, with 3 spam samples over 1966. This means that only 0.15% of the first 1966 ranked hosts are spam, compared to 9.8% over the entire labeled dataset.\n",
    "\n",
    "Another important point is that Robust PageRank returns (for non-spam pages) an ordering similar to the one given by PageRank. A simple way to verify this feature is by intersecting the top 25% sets obtained with Normal and Robust PageRank algorithms. These sets (apart from spam hosts) are almost overlapping. 1717 hosts out of 1966 are present in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spam hosts:  773\n",
      "Total labeled hosts:  7866\n",
      "Total spam hosts %:  9.827 \n",
      "\n",
      "Spam hosts in top 25% (Robust PageRank):  3\n",
      "Labeled hosts in top 25% (Robust PageRank):  1966\n",
      "Spam hosts % in top 25% (Robust PageRank):  0.153 \n",
      "\n",
      "Size of intersection between top 25% with Normal and Robust PageRank:  1717\n"
     ]
    }
   ],
   "source": [
    "robust_labeled_top=(-robust_rank).argsort()[:(n*nl)//100]\n",
    "robust_y_top=y[robust_labeled_top]\n",
    "\n",
    "print(\"Total spam hosts: \", sum(y))\n",
    "print(\"Total labeled hosts: \", len(y))\n",
    "print(\"Total spam hosts %: \", round(100*sum(y)/len(y), 3), '\\n')\n",
    "print(\"Spam hosts in top 25% (Robust PageRank): \", sum(robust_y_top))\n",
    "print(\"Labeled hosts in top 25% (Robust PageRank): \", len(robust_y_top))\n",
    "print(\"Spam hosts % in top 25% (Robust PageRank): \", round(100*sum(robust_y_top)/len(robust_y_top), 3), '\\n')\n",
    "\n",
    "print(\"Size of intersection between top 25% with Normal and Robust PageRank: \", len(np.intersect1d(robust_labeled_top, labeled_top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<a id='references'></a>\n",
    "- [1] R. Andersen, C. Borgs, J. Chayes, J. Hopcroft, K. Jain, V. Mirrokni and S. Teng. [Robust PageRank and locally computable spam detection features](https://dl.acm.org/doi/10.1145/1451983.1452000). In Proceedings of the 4th international workshop on Adversarial information retrieval on the web (AIRWeb '08), 2008.\n",
    "- [2] R. Andersen, C. Borgs, J. Chayes, J. Hopcroft, V. Mirrokni and S. Teng. [Local computation of PageRank contributions](https://link.springer.com/chapter/10.1007/978-3-540-77004-6_12). In 5th International Workshop of Algorithms and Models for the Web-Graph, 2007.\n",
    "- [3] C. Barry, M. Lardner. [A Study of First Click Behaviour and User Interaction on the Google SERP](https://link.springer.com/chapter/10.1007/978-1-4419-9790-6_7). In: Pokorny J. et al. (eds) Information Systems Development. Springer, New York, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
