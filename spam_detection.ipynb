{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web spam detection using link-based features\n",
    "\n",
    "Project for Information Retrieval exam at University of Trieste, January 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlaFEQXfIVoY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from module import * #python module containing custom functions\n",
    "np.random.seed(2) #random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9agASkKvIVo4"
   },
   "source": [
    "Web spam detection is a crucial issue for web search engines. In fact, ranking algorithms such as PageRank do not explicitly penalize spam websites in favor of trustworthy ones, meaning that users may find very high-rank pages that have no useful content and are highly ranked because they are part of a link farm, a popular way to fool ranking algorithms.\\\n",
    "\\\n",
    "The [WEBSPAM-UK2006](https://chato.cl/webspam/datasets/uk2006/) dataset contains 11402 hosts in the `.uk` domain, of which 7866 are labeled as *spam* or *normal*. Newer datasets have been released by the same authors, but this 2006 version remains the one with the highest number of manually labeled samples.\\\n",
    "The file `new_hostnames.csv` contains the names of the hosts in the dataset, while `webspam-uk2006-labels.txt` assigns to 8045 host names a label chosen among *spam*, *normal* or *undecided*. For the purposes of this project, *undecided*-labeled hosts were considered unlabeled, leaving only 7866 hosts labeled as *spam* or *normal*.\\\n",
    "Finally, the file `uk-2006-05.hostgraph_weighted.txt` contains the weighted graph of the hosts, each row containing a host index, the indices of outlinked hosts and, for each of them, the number of outlinks.\\\n",
    "The function `read_graph` returns a sparse `csr_matrix` $\\mathcal{R}$, with $\\mathcal{R}_{i,j}$ equal to the number of outlinks from host $i$ to host $j$ divided by the total number of outlinks of host $i$.\\\n",
    "Since PageRank algorithm requires a stochastic matrix, but there is no guarantee that each host has at least one outlink (dangling node problem), as proposed in Andersen et al., an artificial node with a single self-loop was added to the graph, with ingoing edges from all dangling nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX0xK75NIVo5"
   },
   "outputs": [],
   "source": [
    "hostnames=hostnames_list('data/new_hostnames.csv')\n",
    "labels_dict=labels_dictionary('data/webspam-uk2006/webspam-uk2006-labels.txt')\n",
    "labels, labeled_dataset=make_dataset(labels_dict,hostnames)\n",
    "R=read_graph('data/uk-2006-05.hostgraph_weighted.txt',len(hostnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic approaches to web spam detection: content-based and link-based.\\\n",
    "In a content-based setting the `HTML` code of pages is scanned and some features are computed, usually related to text (for example, average word length or fraction of visible text). On the other hand, in a link-based setting (the one explored in this notebook), information is obtained exclusively from the web graph and computations performed over it (such as ranking algorithms).\\\n",
    "Some very trivial information that can be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h00MQ92fIVo5"
   },
   "source": [
    "For the following computations, it is useful to store the columns of the $\\mathcal{R}$ matrix in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLV0yehHIVo6"
   },
   "outputs": [],
   "source": [
    "columns=columns_list(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCzgDNM_IVpO"
   },
   "source": [
    "In the following cell PageRank is computed iteratively, according to the equation:\n",
    "$$\n",
    "{rank}_{k+1}^T=\\frac{\\alpha}{N}\\mathbf{1}^T+(1-\\alpha)R^T\\cdot {rank}_k^T\n",
    "$$\n",
    "where ${rank}_k$ is the row vector storing the PageRank scores at step $k$, $\\mathbf{1}$ is a row vector of ones, $N$ is the number of nodes (in this case 11403) and $\\alpha$ is the teleporting factor. The iterative computation is performed up to a fixed precision of $\\epsilon$.\\\n",
    "\\\n",
    "Personalized PageRank is an algorithm directly derived from PageRank, whose result is a matrix $\\mathcal{PRM}$ such that $\\mathcal{PRM}_{i,j}$ is the contribution of node $i$ to the PageRank of node $j$. This implies that the sum of $\\mathcal{PRM}$ is equal to the PageRank vector.\\\n",
    "The contribution vector $cpr(v)$ is defined to be the row vector whose transpose is the $v$-th column of matrix $\\mathcal{PRM}$. This vector stores the contribution of all nodes to the PageRank of node $v$ and it is of particular interest when it comes to web spam detection.\\\n",
    "\\\n",
    "However, Personalized PageRank computation is infeasible on large datasets, since it requires an iterative computation (conceptually identical to the one of PageRank) that includes, at each step, a matrix multiplication between two square $N \\times N$ matrices, one of which is non-sparse.\\\n",
    "To address this problem, Andersen et al. propose a local algorithm for the computation of $\\delta$-approximations of contribution vectors.\n",
    "\\\n",
    "Given a node $v$ and its contribution vector $c:=cpr(v)$, a $\\delta$-approximation of $cpr(v)$ is a non-negative vector $c^*$ such that $c(u)-\\delta rank(v) \\leq c^*(u) \\leq c(u)$ for all nodes $u$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUy9RdLnIVpP",
    "outputId": "fa93a50b-58b4-476a-ed33-16ea20e10352"
   },
   "outputs": [],
   "source": [
    "alpha=.01\n",
    "eps=1e-8\n",
    "delta=1e-3\n",
    "\n",
    "print(\"PageRank computation\")\n",
    "rank=compute_PR(alpha,eps,R)\n",
    "nl=len(labeled_dataset)\n",
    "ap=np.zeros((nl,len(rank)))\n",
    "print(\"Approximation of Personalized PageRank for labeled hosts\")\n",
    "for v in range(nl):\n",
    "    print(str(v)+'/'+str(nl),end='\\r')\n",
    "    ap[v]=(approximate_contributions(labeled_dataset[v], alpha, delta*rank[labeled_dataset[v]], rank[labeled_dataset[v]], columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvYT-RqIMagT"
   },
   "source": [
    "Once a $\\delta$-approximation is computed for the contribution vector of a node $v$, it is possible to compute some useful features:\n",
    " - Size of $\\delta$-significant contributing set: for a node $v$, this feature is defined as $|S_{\\delta}|=|\\{u|c^*(u)>\\delta rank(v)\\}|$.\n",
    " - Contribution from vertices in the $\\delta$-significant contributing set: $\\sum_{u \\in S_{\\delta}} \\frac{c^*(u)}{rank(v)}$ \n",
    " - $l_2$ norm of $\\delta$-significant contributing vector: $\\sum_{u \\in S_{\\delta}} (\\frac{c^*(u)}{rank(v)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cnjr-v6ZIVpb"
   },
   "outputs": [],
   "source": [
    "x=extract_features(R,delta,ap,labeled_dataset,rank)\n",
    "y=labels[labeled_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cs7mwdBIVpb"
   },
   "source": [
    "Spam detection is particularly relevant for high PageRank hosts, since people tend to click on highly ranked pages, often within the first page of search engine results.\\\n",
    "However, we can see that PageRank alone is not able to filter out spam pages. In fact, if we restrict our view to the highest ranked 25% of the dataset, 161 hosts out of 2095 (total labeled hosts in this 25%) are labeled as spam, as opposed to 773/7866 on the entire dataset. The proportion of spam hosts drops from 9.8% to 7.7%, but that is surely not enough to consider PageRank as a spam detection or spam-robust algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGIMhttnIVpc",
    "outputId": "21caa124-fd33-4993-f4c5-c33c69a150e6"
   },
   "outputs": [],
   "source": [
    "n=25\n",
    "labeled_top=top_n_percent(n,rank,labeled_dataset)\n",
    "\n",
    "x_top=x[labeled_top]\n",
    "y_top=y[labeled_top]\n",
    "\n",
    "print(\"Spam hosts in top 25%: \", sum(y_top))\n",
    "print(\"Total labeled hosts in top 25%: \", len(y_top))\n",
    "print(\"Total spam hosts: \", sum(y))\n",
    "print(\"Total labeled hosts: \",len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTE-XdduQcuW"
   },
   "source": [
    "Some Machine Learning can be used to detect spam websites. The simplest approach would be to consider one individual feature, set a threshold and classify as *spam* or *normal* all hosts with a score above (or below) that threshold.\\\n",
    "Actually, despite being trivial, this approach works quite well in this case, since it allows to classify labeled hosts with an overall accuracy of 70% and, most importantly, with a recall of 95% on the *spam* class, for which one could imagine recall to be crucial over precision.\\\n",
    "However, more complex techniques can be used to increase accuracy and precision, while trying not to compromise too much recall.\\\n",
    "Simple tree-based models seem to work well on these data. In fact, a decision tree classifier scores an accuracy just below 90% and a random forest classifier scores 92% accuracy. However, one has to keep in mind that these models have much lower recall on *spam* class, scoring respectively 44% and 47%, meaning that more than 1 spam website out of 2 is classified as a *normal* website.\\\n",
    "Similar results can be achieved on the set of top 25% highest ranked hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONBwFzjGIVpd",
    "outputId": "4c089661-8048-4cd2-db32-49c5038f2493"
   },
   "outputs": [],
   "source": [
    "print(\"Single split decision tree\")\n",
    "clf=DecisionTreeClassifier(max_depth=1,class_weight='balanced')\n",
    "pred=cross_val_predict(clf,x,y,cv=10)\n",
    "print(\"Accuracy: \", accuracy(y,pred))\n",
    "print(\"Precision on spam: \", precision(y,pred,average=None)[1])\n",
    "print(\"Recall on spam: \",recall(y,pred,average=None)[1])\n",
    "\n",
    "print(\"Decision tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "pred=cross_val_predict(clf,x,y,cv=10)\n",
    "print(\"Accuracy: \", accuracy(y,pred))\n",
    "print(\"Precision on spam: \", precision(y,pred,average=None)[1])\n",
    "print(\"Recall on spam: \",recall(y,pred,average=None)[1])\n",
    "\n",
    "print(\"Random forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced_subsample')\n",
    "pred=cross_val_predict(clf,x,y,cv=10)\n",
    "print(\"Accuracy: \", accuracy(y,pred))\n",
    "print(\"Precision on spam: \", precision(y,pred,average=None)[1])\n",
    "print(\"Recall on spam: \",recall(y,pred,average=None)[1])\n",
    "\n",
    "\n",
    "print(\"Single split decision tree\")\n",
    "clf=DecisionTreeClassifier(max_depth=1,class_weight='balanced')\n",
    "pred=cross_val_predict(clf,x_top,y_top,cv=10)\n",
    "print(\"Accuracy: \", accuracy(y_top,pred))\n",
    "print(\"Precision on spam: \", precision(y_top,pred,average=None)[1])\n",
    "print(\"Recall on spam: \",recall(y_top,pred,average=None)[1])\n",
    "\n",
    "print(\"Decision tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "pred=cross_val_predict(clf,x_top,y_top,cv=10)\n",
    "print(\"Accuracy: \", accuracy(y_top,pred))\n",
    "print(\"Precision on spam: \", precision(y_top,pred,average=None)[1])\n",
    "print(\"Recall on spam: \",recall(y_top,pred,average=None)[1])\n",
    "\n",
    "print(\"Random forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced_subsample')\n",
    "pred=cross_val_predict(clf,x_top,y_top,cv=10)\n",
    "print(\"Accuracy: \", accuracy(y_top,pred))\n",
    "print(\"Precision on spam: \", precision(y_top,pred,average=None)[1])\n",
    "print(\"Recall on spam: \",recall(y_top,pred,average=None)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
