{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web spam detection through link-based features\n",
    "\n",
    "Project for Information Retrieval exam at University of Trieste, January 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nlaFEQXfIVoY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import spam_detection as sd #python module containing custom functions\n",
    "np.random.seed(2) #random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9agASkKvIVo4"
   },
   "source": [
    "Web spam detection is a crucial issue for web search engines. In fact, ranking algorithms such as PageRank cannot explicitly penalize spam websites in favor of trustworthy ones, meaning that users may find very high-rank pages that have no useful content and are highly ranked because they are part of a link farm, a popular way to fool ranking algorithms.\n",
    "\n",
    "###  The dataset\n",
    "The [WEBSPAM-UK2006](https://chato.cl/webspam/datasets/uk2006/) dataset contains 11402 hosts in the `.uk` domain, of which 7866 are labeled as *spam* or *normal*. Newer datasets have been released by the same authors, but this 2006 version remains the one with the highest number of manually labeled samples.\n",
    "\n",
    "The file `new_hostnames.csv` contains the names of the hosts in the dataset, while `webspam-uk2006-labels.txt` assigns to 8045 host names a label chosen among *spam*, *normal* or *undecided*. For the purposes of this project, *undecided*-labeled hosts were considered unlabeled, leaving only 7866 hosts labeled as *spam* or *normal*.\n",
    "\n",
    "Finally, the file `uk-2006-05.hostgraph_weighted.txt` contains the weighted graph of the hosts, each row containing a host index, the indices of outlinked hosts and, for each of them, the number of outlinks.\n",
    "\n",
    "The function `read_graph` returns a sparse `csr_matrix` $R^T$, with $R[i,j]$ equal to $0$ if there is no edge connecting host $i$ to host $j$ or to $\\frac{1}{O[i]}$ where $O[i]$ is the total number of hosts outlinked by $i$.\n",
    "\n",
    "Since PageRank algorithm requires $R$ to be a stochastic matrix, but there is no guarantee that each host has at least one outlink (dangling node problem), as proposed in [[1]](#references), an artificial node with a single self-loop was added to the graph, with ingoing edges from all dangling nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IX0xK75NIVo5"
   },
   "outputs": [],
   "source": [
    "hostnames=sd.hostnames_list('data/new_hostnames.csv')\n",
    "labels_dict=sd.labels_dictionary('data/webspam-uk2006-labels.txt')\n",
    "labels, labeled_dataset=sd.make_dataset(labels_dict,hostnames)\n",
    "R_T=sd.read_graph('data/uk-2006-05.hostgraph_weighted.txt',len(hostnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCzgDNM_IVpO"
   },
   "source": [
    "### PageRank\n",
    "\n",
    "In the following cell PageRank is computed iteratively, according to the equation:\n",
    "\n",
    "$$\n",
    "{rank}_{k+1}=\\frac{\\alpha}{N}\\mathbf{1}+(1-\\alpha)R^T\\cdot {rank}_k\n",
    "$$\n",
    "\n",
    "where ${rank}_k$ is the column vector storing the PageRank scores at step $k$, $\\mathbf{1}$ is a column vector of ones, $N$ is the number of nodes (in this case 11403) and $\\alpha$ is the teleporting factor. The iterative computation is performed up to a fixed precision of $\\epsilon$, i.e. until $|{rank}_k-{rank}_{k-1}|_1 < \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank computation\n",
      "PageRank computed\n"
     ]
    }
   ],
   "source": [
    "alpha=.1\n",
    "eps=1e-8\n",
    "\n",
    "print(\"PageRank computation\")\n",
    "rank=sd.compute_PR(alpha,eps,R_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An approximation for PageRank contributions\n",
    "\n",
    "For the purpose of spam detection, it is useful to have a measure of the individual contributions of other hosts to the PageRank of a given host. This means obtaining a $N \\times N$ matrix $PRM$ such that $PRM[i,j]$ is the contribution of node $i$ to the PageRank of node $j$ and such that the sum over the columns of $PRM$ is equal to the PageRank vector.\n",
    "\n",
    "To do so, one possibility (explored in [[1]](#references)) is to use topic-specific PageRank $N$ times with jump vectors belonging to the standard basis of $\\mathbb{R}^N$. With this assumption, each row of the $PRM$ satisfies the equation:\n",
    "$$\n",
    "PRM[u]=\\alpha \\mathbf{e_u}+(1-\\alpha)PRM[u] \\cdot R\n",
    "$$\n",
    "Where $\\mathbf{e_u}$ is the $u$-th row vector of the standard basis of $\\mathbb{R}^N$.\n",
    "\n",
    "Then, given a node index $v$, its corresponding contribution vector $cpr[v]$ is defined to be the row vector whose transpose is the $v$-th column of matrix $PRM$. This vector stores the contribution of all nodes to the PageRank of node $v$ and it will prove to be of particular interest when it comes to web spam detection.\n",
    "\n",
    "However, computing topic-specific PageRank $N$ times is infeasible on large datasets, since each step requires an iterative computation conceptually identical to the one of PageRank.\n",
    "\n",
    "To address this problem, the authors of [[2]](#references) propose a local algorithm for the computation of $\\delta$-approximations of contribution vectors.\n",
    "\n",
    "Given a node $v$ and its contribution vector $c:=cpr[v]$, a $\\delta$-approximation of $c$ is a non-negative vector $c^*$ such that: \n",
    "\n",
    "$$\n",
    "c[u]-\\delta rank[v] \\leq c^*[u] \\leq c[u]\\space \\space \\space \\forall u \\in \\{0,...,N-1\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hUy9RdLnIVpP",
    "outputId": "fa93a50b-58b4-476a-ed33-16ea20e10352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of Personalized PageRank for labeled hosts\n",
      "7866/7866\r"
     ]
    }
   ],
   "source": [
    "delta=1e-3\n",
    "\n",
    "nl=len(labeled_dataset)\n",
    "cpr=np.zeros((nl,len(rank)))\n",
    "print(\"Approximation of Personalized PageRank for labeled hosts\")\n",
    "\n",
    "'''\n",
    "approximate_contributions makes repeated access to the rows of R_T. Scipy sparse matrices are much slower than numpy arrays for\n",
    "slicing, hence it is much more efficient to convert R_T to numpy for this computation. However, in the case of a larger dataset,\n",
    "this step might not be feasible because of memory limitations.\n",
    "'''\n",
    "\n",
    "R_T_np=R_T.toarray()\n",
    "\n",
    "for v in range(nl):\n",
    "    print(str(v+1)+'/'+str(nl),end='\\r')\n",
    "    cpr[v]=(sd.approximate_contributions(labeled_dataset[v], alpha, delta*rank[labeled_dataset[v]], rank[labeled_dataset[v]], R_T_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cs7mwdBIVpb"
   },
   "source": [
    "### Features for link-based web spam detection\n",
    "Spam detection is particularly relevant for high PageRank hosts, since people tend to click on highly ranked pages, almost always within the first page of search engine results [[3]](#references).\n",
    "\n",
    "However, we can see that PageRank alone is not able to filter out spam pages. In fact, if we restrict our view to the highest ranked 25% of the dataset, 144 hosts out of 2276 (total labeled hosts in this 25%) are labeled as spam, as opposed to 773/7866 on the entire dataset. The proportion of spam hosts drops from 9.8% to 6.3%, but that is surely not enough to consider PageRank as a spam detection or spam-robust algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FGIMhttnIVpc",
    "outputId": "21caa124-fd33-4993-f4c5-c33c69a150e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spam hosts:  773\n",
      "Total labeled hosts:  7866\n",
      "Total spam hosts %:  9.827103991863718 \n",
      "\n",
      "Spam hosts in top 25%:  147\n",
      "Labeled hosts in top 25%:  2242\n",
      "Spam hosts % in top 25%:  6.5566458519179305\n"
     ]
    }
   ],
   "source": [
    "n=25\n",
    "labeled_top=sd.top_n_percent(n,rank,labeled_dataset)\n",
    "\n",
    "y=labels[labeled_dataset]\n",
    "y_top=y[labeled_top]\n",
    "\n",
    "print(\"Total spam hosts: \", sum(y))\n",
    "print(\"Total labeled hosts: \",len(y))\n",
    "print(\"Total spam hosts %: \", 100*sum(y)/len(y),'\\n')\n",
    "print(\"Spam hosts in top 25%: \", sum(y_top))\n",
    "print(\"Labeled hosts in top 25%: \", len(y_top))\n",
    "print(\"Spam hosts % in top 25%: \", 100*sum(y_top)/len(y_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic approaches to web spam detection: content-based and link-based.\n",
    "\n",
    "In a content-based setting the `HTML` code of pages is scanned and some features are computed, usually related to text (for example, average word length or fraction of visible text). On the other hand, in a link-based setting (the one explored in this notebook), information is obtained exclusively from the web graph and computations performed over it (such as ranking algorithms).\n",
    "\n",
    "Some very trivial link-based features that can be computed even before performing PageRank are indegree and outdegree, defined as follows:\n",
    " - Indegree: total number of incoming links to a host\n",
    " - Outdegree: total number of outgoing links from a host\n",
    "\n",
    "Other useful features can be computed from the contribution vector of a node (or from a $\\delta$-approximation):\n",
    " - Size of $\\delta$-significant contributing set: for a node $v$, this feature is defined as $cs\\_size=|S_{\\delta}|=|\\{u|c^*(u)>\\delta rank(v)\\}|$.\n",
    " - Contribution from vertices in the $\\delta$-significant contributing set: $cs\\_contribution=\\sum_{u \\in S_{\\delta}} \\frac{c^*(u)}{rank(v)}$ \n",
    " - $l_2$ norm of $\\delta$-significant contributing vector: $l_2\\_norm=\\sum_{u \\in S_{\\delta}} (\\frac{c^*(u)}{rank(v)})^2$\n",
    "\n",
    "These features, together with PageRank scores, can be used to train a binary classifier for spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cnjr-v6ZIVpb"
   },
   "outputs": [],
   "source": [
    "x=np.zeros((nl,6))\n",
    "indegree,outdegree,cs_size,cs_contribution,l2_norm=sd.extract_features(R_T_np,delta,cpr,labeled_dataset,rank)\n",
    "x[:,0]=rank[labeled_dataset]\n",
    "x[:,1]=indegree\n",
    "x[:,2]=outdegree\n",
    "x[:,3]=cs_size\n",
    "x[:,4]=cs_contribution\n",
    "x[:,5]=l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTE-XdduQcuW"
   },
   "source": [
    "### Evaluation of classifiers\n",
    "\n",
    "Some Machine Learning can be used to detect spam websites. Three different approaches are compared: Logistic Regression, Decision Tree and Random Forest. For all these classifiers, the implementation from `scikit-learn` library is used with its default hyperparameter values, with the exception of the parameter `class_weight`, which is set to `balanced` to try to mitigate the unbalance in the dataset between *spam* and *normal* labeled data points. \n",
    "\n",
    "The fit and test of these models is performed through a 10-fold cross validation. Regarding performance metrics, in a classification setting accuracy is obviously to be taken into account but, in this case, not sufficient to compare the models. In fact, given how unbalanced the dataset is, even a naive classifier that classifies all hosts as *normal* would achieve over 90% accuracy despite being completely useless for spam detection. To solve this problem, class-specific precision and recall statistics are taken into account for *spam* class. In particular, since spam labeling might be safety critical, one could imagine recall to be particularly relevant.\n",
    "\n",
    "Tree-based classifiers work definitely better than Logistic Regression, scoring accuracies over 90% while attaining reasonable values for precision and recall on *spam* samples. However, Logistic Regression scores an outstading 95% recall on *spam* meaning that it succeeds in retrieving almost all spam hosts (at the cost of many false alarms, captured by low precision score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ONBwFzjGIVpd",
    "outputId": "4c089661-8048-4cd2-db32-49c5038f2493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy:  0.7077294685990339\n",
      "Precision on spam:  0.24634308510638298\n",
      "Recall on spam:  0.9586028460543338\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.9093567251461988\n",
      "Precision on spam:  0.5402144772117963\n",
      "Recall on spam:  0.5213454075032341\n",
      "\n",
      "Random Forest\n",
      "Accuracy:  0.9326214085939486\n",
      "Precision on spam:  0.7048903878583473\n",
      "Recall on spam:  0.5407503234152652\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "clf=LogisticRegression(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x,y,k)\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x,y,k)\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced_subsample')\n",
    "sd.print_prediction_metrics(clf,x,y,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it is particularly interesting to restrict the view to the top 25% highest ranked hosts, since these are the ones that users are most likely to run into.\n",
    "\n",
    "Here there is no huge gap in accuracy between the three classifiers, with values well over 95% for all of them. Tree-based methods confirm their higher precision on *spam* samples but, because of the different misclassification cost between the two class and because of a (almost) perfect recall score on *spam* Logistic Regression would probably be preferrable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy:  0.9393398751115076\n",
      "Precision on spam:  0.5197132616487455\n",
      "Recall on spam:  0.9863945578231292\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.9638715432649421\n",
      "Precision on spam:  0.7323943661971831\n",
      "Recall on spam:  0.7074829931972789\n",
      "\n",
      "Random Forest\n",
      "Accuracy:  0.971900089206066\n",
      "Precision on spam:  0.8\n",
      "Recall on spam:  0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "x_top=x[labeled_top]\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "clf=LogisticRegression(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x_top,y_top,k)\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "clf=DecisionTreeClassifier(class_weight='balanced')\n",
    "sd.print_prediction_metrics(clf,x_top,y_top,k)\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "clf=RandomForestClassifier(class_weight='balanced_subsample')\n",
    "sd.print_prediction_metrics(clf,x_top,y_top,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<a id='references'></a>\n",
    "- [1] R. Andersen, C. Borgs, J. Chayes, J. Hopcroft, K. Jain, V. Mirrokni and S. Teng. [Robust PageRank and locally computable spam detection features](https://dl.acm.org/doi/10.1145/1451983.1452000). In Proceedings of the 4th international workshop on Adversarial information retrieval on the web (AIRWeb '08), 2008.\n",
    "- [2] R. Andersen, C. Borgs, J. Chayes, J. Hopcroft, V. Mirrokni and S. Teng. [Local computation of PageRank contributions](https://link.springer.com/chapter/10.1007/978-3-540-77004-6_12). In 5th International Workshop of Algorithms and Models for the Web-Graph, 2007.\n",
    "- [3] C. Barry, M. Lardner. [A Study of First Click Behaviour and User Interaction on the Google SERP](https://link.springer.com/chapter/10.1007/978-1-4419-9790-6_7). In: Pokorny J. et al. (eds) Information Systems Development. Springer, New York, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
